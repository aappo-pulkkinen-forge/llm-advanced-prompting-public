{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be94e6d6-4096-4d1a-aa58-5afd89f33bff",
   "metadata": {},
   "source": [
    "# Tokenizers and models\n",
    "\n",
    "Let's begin with testing how to use tokenizers and models from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261c234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install openai\n",
    "%pip install scikit-learn\n",
    "%pip install numpy\n",
    "%pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8ea85-d04d-4217-99a3-21c446bf2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline\n",
    ")\n",
    "from typing import List\n",
    "from datasets import load_dataset\n",
    "from openai import AzureOpenAI\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import pipeline\n",
    "import os\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea71709",
   "metadata": {},
   "source": [
    "# Let's test text generation with different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a5113b",
   "metadata": {},
   "source": [
    "### Load GPT-2 model and tokenizer from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b0585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the gpt-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the gpt-2 model with the text generation head\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2321385d",
   "metadata": {},
   "source": [
    "### Try out the loaded tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e90cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding can be done with encode method\n",
    "input_text = \"The most important thing in life is\"\n",
    "print(\"Input text was: \", input_text, \"\\n\")\n",
    "\n",
    "encoded_input = tokenizer.encode(input_text)\n",
    "print(\"Encoded input:\", encoded_input, \"\\n\")\n",
    "\n",
    "# Decoding can be done with the decode method\n",
    "# When decoding the encoded input, the tokenizer should return the original text.\n",
    "decoded_input = tokenizer.decode(encoded_input)\n",
    "print(\"Decoding the tokens back to original input: \", decoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbaddba",
   "metadata": {},
   "source": [
    "### Try out the loaded GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0463f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference can be done by calling .generate method of the model\n",
    "model_output = gpt2_model.generate(**tokenizer(input_text, return_tensors=\"pt\"), max_new_tokens=10)\n",
    "\n",
    "print(\"Model output is just tokens:\")\n",
    "print(model_output[0])\n",
    "\n",
    "print(\"\\nModel output needs to be decoded with the tokenizer to get meaningful words:\")\n",
    "print(tokenizer.decode(model_output[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916ac69",
   "metadata": {},
   "source": [
    "### TODO\n",
    "The above output was somewhat reasonable with GPT-2 model. What if you increase the number of `max_new_tokens`.\n",
    "\n",
    "Try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706462f8",
   "metadata": {},
   "source": [
    "### Try out a model trained for classification\n",
    "\n",
    "The previous GPT-2 model was trained for Causal Language Modelling task, .i.e. to predict the text continuation. Let's try out a model trained for classification task.\n",
    "\n",
    "ProsusAI/finbert model description:\n",
    "\n",
    "\"FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f77bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the finbert tokenizer\n",
    "finbert_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "# Load the finbert model with the text generation head\n",
    "finbert_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae00d4f",
   "metadata": {},
   "source": [
    "### Try out the classification model\n",
    "\n",
    "Notice that calling the model happens now with model callable, not with .generate method, and `max_new_tokens` input parameters does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Top private equity firms put brakes on China dealmaking\"\n",
    "model_output = finbert_model(**finbert_tokenizer(input_text, return_tensors=\"pt\"))\n",
    "print(\"Model output (for positive, negative or neutral sentiment):\")\n",
    "print(model_output[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc36191a",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "1. Make sure you understand the model output.\n",
    "2. Try out the finbert model some more and test it with some other input. Do you find some examples for which it would output faulty classification (sentiment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c4e9f",
   "metadata": {},
   "source": [
    "### Let's test some more advanced models through Azure API's\n",
    "\n",
    "It's easy to deploy models to cloud by using any of the LLM API providers. Let's test how to run models deployd using Azure AI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e80da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Insert the provided API key here\n",
    "api_key_gpt4o = os.getenv(\"AZURE_GPT4O_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838e0cf",
   "metadata": {},
   "source": [
    "GPT-4o mini is specifically built for chat, so the deployed model has a \"chat/completions\" endpoint. Notice that also the the input has pre-defined structure containing a list of messages each of which have \"role\" and \"content\" fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02baf667",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name=\"gpt-4o-mini\"\n",
    "api_version=\"2024-08-01-preview\"\n",
    "task = \"chat/completions\"\n",
    "endpoint = f\"https://aiservices-forge-test-westeu.openai.azure.com/\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key_gpt4o,  \n",
    "    api_version=api_version,\n",
    "    azure_endpoint = endpoint\n",
    "    )\n",
    "input = \"The best way to learn how to build RAG applications is to \"\n",
    "\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me four basic ingredients for crepes. Answer only with a list of ingredients.\"},\n",
    "]\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=messages\n",
    ")\n",
    "chat_completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b8d04b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Insert the provided API key here\n",
    "api_key_gpt35 = os.getenv(\"AZURE_GPT35_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffce85c",
   "metadata": {},
   "source": [
    "GPT-3.5 model is trained for causal langauge modelling (text continuation) and the deployed model has a \"completions\" endpoint for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version=\"2024-02-01\"\n",
    "endpoint = \"https://aiservices-forge-test-swe.openai.azure.com/\"\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key_gpt35,  \n",
    "    api_version=api_version,\n",
    "    azure_endpoint = endpoint\n",
    "    )\n",
    "\n",
    "input = \"Basic ingredients for crepes are: \"\n",
    "response = client.completions.create(model=\"gpt-35-turbo-instruct\", prompt=input, max_tokens=50)\n",
    "\n",
    "print(f\"Input: {input}\")\n",
    "print(f\"Response: {response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b85ec0",
   "metadata": {},
   "source": [
    "You can also deploy models for text embeddings. Let's try one out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fa49b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Insert the provided API key here\n",
    "api_key_embedding = os.getenv(\"AZURE_EMBEDDINS_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa5ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: deploy this\n",
    "\n",
    "deployment_name=\"text-embedding-3-large\"\n",
    "api_version=\"2023-05-15\"\n",
    "endpoint = \"https://aiservices-forge-test-swe.openai.azure.com/\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key_embedding,  \n",
    "    api_version=api_version,\n",
    "    azure_endpoint = endpoint\n",
    "    )\n",
    "    \n",
    "input = \"Some text to generate embeddings for.\"\n",
    "response = client.embeddings.create(model=deployment_name, input=input)\n",
    "\n",
    "print(f\"Input: {input}\")\n",
    "print(f\"Response: {response.data[0].embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad403c6b",
   "metadata": {},
   "source": [
    "Suggestions for things to try out later on:\n",
    "1. Search Huggingface for some models that looks interesting and try them out. You can also use th Huggingface portal \"Inference API\" directly if you want.\n",
    "2. Test different embedding models. Can mix & match different models i.e. are the embeddings somehow comparable accross different models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd23c78",
   "metadata": {},
   "source": [
    "### HuggingFace pipeline\n",
    "\n",
    "HuggingFace also has convenient `pipeline` abstraction for model inference. It offers a simple API for running the models without the need to load for instance tokenizers separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipe = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
    "\n",
    "input_text = \"Top private equity firms put brakes on China dealmaking\"\n",
    "pipe(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da0a26",
   "metadata": {},
   "source": [
    "# Embeddings and RAG\n",
    "\n",
    "Let's next build a very simple RAG application. The application uses financial new articles as a database and is able to find similar articles to a given one and generate some additional information regarding the retrieved articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8899f19",
   "metadata": {},
   "source": [
    "### Load a dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fina_news = load_dataset(\"Aappo/fina_news_1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21bde6",
   "metadata": {},
   "source": [
    "The loaded dataset contains financial news data (news headline, journalists, data, link to the article and the article text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd6b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fina_news['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc0a534",
   "metadata": {},
   "source": [
    "We will use an embedding model from HuggingFace. Embedding models can be loaded by using the SentenceTransformer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2f53165",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"msmarco-distilbert-base-v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c2f34",
   "metadata": {},
   "source": [
    "### Some helper functions\n",
    "\n",
    "Let's define some helper functions for generating a vector index and for searching the index. In this example case the vector index is a scikit-learn nearest neighbour model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "191b46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_documents_huggingface(articles:List[str]):\n",
    "    embeddings = embedder.encode(articles)\n",
    "    nbrs = NearestNeighbors(n_neighbors=5, algorithm='kd_tree').fit(embeddings)\n",
    "    return nbrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be1bdf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbours_huggingface(nbrs, article:str, all_articles: List[str], n_neighbors:int=2):\n",
    "    embedding = embedder.encode(article)\n",
    "    neighbour_indices = nbrs.kneighbors([embedding], n_neighbors=n_neighbors)\n",
    "    neighbour_artices = np.array(all_articles)[neighbour_indices[1][0]]\n",
    "    return neighbour_artices, neighbour_indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d7a54",
   "metadata": {},
   "source": [
    "### Let's index the articles\n",
    "\n",
    "This can take a short while on Colab, so we are only using the first 100 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db7e6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs_huggingface = index_documents_huggingface(fina_news[\"train\"][\"Article\"][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e2a9b",
   "metadata": {},
   "source": [
    "### Find the similar articles of a given one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b18317",
   "metadata": {},
   "source": [
    "Let's take a random article from our article catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = fina_news[\"train\"][\"Article\"][10]\n",
    "display(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e67e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nearest_articles = get_nearest_neighbours_huggingface(nbrs=nbrs_huggingface, all_articles=fina_news[\"train\"][\"Article\"][:1000], article=article, n_neighbors=5)\n",
    "display(nearest_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf11a13",
   "metadata": {},
   "source": [
    "### Generate some additional information about the retrieved articles\n",
    "\n",
    "Let' start with generating short summaries of the retrieved articles. There are specialized summarization models as well, but we'll use prompting and GPT-4o model in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a43220",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name=\"gpt-4o-mini\"\n",
    "api_version=\"2024-08-01-preview\"\n",
    "endpoint = f\"https://aiservices-forge-test-westeu.openai.azure.com/\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key_gpt4o,  \n",
    "    api_version=api_version,\n",
    "    azure_endpoint = endpoint\n",
    "    )\n",
    "input = \"The best way to learn how to build RAG applications is to \"\n",
    "\n",
    "\n",
    "for article in nearest_articles[0]:\n",
    "    messages = [{\"role\":\"system\", \"content\": \"You are a helpful assistant giving short one sentence summary of the given text.\"},\n",
    "                {\"role\": \"user\", \"content\": article}]\n",
    "    response = client.chat.completions.create(model=deployment_name, messages=messages, max_tokens=100)\n",
    "    print(\"\\nSummary:\")\n",
    "    display(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf2e80",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "You can continue to develop this application further:\n",
    "\n",
    "1. How could you use the GPT-4o model to classify the articles based on for instance their topic or sentiment?\n",
    "2. How could you change the prompt to use GPT-4o to explain why the articles are similar to each other?\n",
    "3. What if you use the above `ProsusAI/finbert` model for classification? If there are errors, how could you prevent those?\n",
    "4. In what type of real life scenario could you use this type of retrieval setup?\n",
    "5. Modify the code so that you use the model `text-embedding-3-large` for generating the embeddings.\n",
    "6. Try deploying your own LLM model on some API provider infra and use that to 1. generate the embeddings 2. generate the additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977383a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
