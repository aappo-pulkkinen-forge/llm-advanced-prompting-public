{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be94e6d6-4096-4d1a-aa58-5afd89f33bff",
   "metadata": {},
   "source": [
    "# Tokenizers and models\n",
    "\n",
    "Let's begin with testing how to use tokenizers and models from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install openai\n",
    "%pip install scikit-learn\n",
    "%pip install numpy\n",
    "%pip install sentence_transformers\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8ea85-d04d-4217-99a3-21c446bf2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline\n",
    ")\n",
    "from typing import List\n",
    "from datasets import load_dataset\n",
    "from openai import AzureOpenAI\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import pipeline\n",
    "import os\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea71709",
   "metadata": {},
   "source": [
    "# Let's test text generation with different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a5113b",
   "metadata": {},
   "source": [
    "### Load GPT-2 model and tokenizer from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b0585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the gpt-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the gpt-2 model with the text generation head\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2321385d",
   "metadata": {},
   "source": [
    "### Try out the loaded tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e90cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding can be done with encode method\n",
    "input_text = \"The most important thing in life is\"\n",
    "print(\"Input text was: \", input_text, \"\\n\")\n",
    "\n",
    "encoded_input = tokenizer.encode(input_text)\n",
    "print(\"Encoded input:\", encoded_input, \"\\n\")\n",
    "\n",
    "# Decoding can be done with the decode method\n",
    "# When decoding the encoded input, the tokenizer should return the original text.\n",
    "decoded_input = tokenizer.decode(encoded_input)\n",
    "print(\"Decoding the tokens back to original input: \", decoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbaddba",
   "metadata": {},
   "source": [
    "### Try out the loaded GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0463f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference can be done by calling .generate method of the model\n",
    "model_output = gpt2_model.generate(**tokenizer(input_text, return_tensors=\"pt\"), max_new_tokens=10)\n",
    "\n",
    "print(\"Model output is just tokens:\")\n",
    "print(model_output[0])\n",
    "\n",
    "print(\"\\nModel output needs to be decoded with the tokenizer to get meaningful words:\")\n",
    "print(tokenizer.decode(model_output[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916ac69",
   "metadata": {},
   "source": [
    "### TODO\n",
    "The above output was somewhat reasonable with GPT-2 model. What if you increase the number of `max_new_tokens`.\n",
    "\n",
    "Try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706462f8",
   "metadata": {},
   "source": [
    "### Try out a model trained for classification\n",
    "\n",
    "The previous GPT-2 model was trained for Causal Language Modelling task, .i.e. to predict the text continuation. Let's try out a model trained for classification task.\n",
    "\n",
    "lvwerra/distilbert-imdb model is trained to classify text based on it's sentiment (positive, negative). It's finetuned by using Imdb movie reviews data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classification model tokenizer\n",
    "classification_tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\")\n",
    "\n",
    "# Load the classification model with the text generation head\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(\"lvwerra/distilbert-imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae00d4f",
   "metadata": {},
   "source": [
    "### Try out the classification model\n",
    "\n",
    "Notice that calling the model happens now with model callable, not with .generate method, and `max_new_tokens` input parameters does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"I love this movie. It was great!\"\n",
    "model_output = classification_model(**classification_tokenizer(input_text, return_tensors=\"pt\"))\n",
    "print(\"Model output (for positive, negative or neutral sentiment):\")\n",
    "print(model_output[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc36191a",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "1. Make sure you understand the model output.\n",
    "2. Try out the finbert model some more and test it with some other input. Do you find some examples for which it would output faulty classification (sentiment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a01fb2",
   "metadata": {},
   "source": [
    "### HuggingFace pipeline\n",
    "\n",
    "HuggingFace also has convenient `pipeline` abstraction for model inference. It offers a simple API for running the models without the need to load for instance tokenizers separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a10863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipe = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\")\n",
    "\n",
    "input_text = \"I love this movie. It was great!\"\n",
    "pipe(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c4e9f",
   "metadata": {},
   "source": [
    "### Let's test some more advanced models through Azure API's\n",
    "\n",
    "It's easy to deploy models to cloud by using any of the LLM API providers. Let's test how to run models deployd using Azure AI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e80da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Insert the provided API key here\n",
    "api_key_gpt4o = os.getenv(\"AZURE_GPT4O_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838e0cf",
   "metadata": {},
   "source": [
    "GPT-4o mini is specifically built for chat, so the deployed model has a \"chat/completions\" endpoint. Notice that also the the input has pre-defined structure containing a list of messages each of which have \"role\" and \"content\" fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02baf667",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name=\"gpt-4o-mini\"\n",
    "api_version=\"2024-08-01-preview\"\n",
    "task = \"chat/completions\"\n",
    "endpoint = f\"https://aiservices-forge-test-westeu.openai.azure.com/\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key_gpt4o,  \n",
    "    api_version=api_version,\n",
    "    azure_endpoint = endpoint\n",
    "    )\n",
    "input = \"The best way to learn how to build RAG applications is to \"\n",
    "\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me four basic ingredients for crepes. Answer only with a list of ingredients.\"},\n",
    "]\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=messages\n",
    ")\n",
    "chat_completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8d04b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Insert the provided API key here\n",
    "api_key_gpt35 = os.getenv(\"AZURE_GPT35_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffce85c",
   "metadata": {},
   "source": [
    "GPT-3.5 model is trained for causal langauge modelling (text continuation) and the deployed model has a \"completions\" endpoint for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version=\"2024-02-01\"\n",
    "endpoint = \"https://aiservices-forge-test-swe.openai.azure.com/\"\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key_gpt35,  \n",
    "    api_version=api_version,\n",
    "    azure_endpoint = endpoint\n",
    "    )\n",
    "\n",
    "input = \"Basic ingredients for crepes are: \"\n",
    "response = client.completions.create(model=\"gpt-35-turbo-instruct\", prompt=input, max_tokens=50)\n",
    "\n",
    "print(f\"Input: {input}\")\n",
    "print(f\"Response: {response.choices[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b85ec0",
   "metadata": {},
   "source": [
    "You can also deploy models for text embeddings. Let's try one out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa49b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Insert the provided API key here\n",
    "api_key_embedding = os.getenv(\"AZURE_EMBEDDINS_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa5ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: deploy this\n",
    "\n",
    "deployment_name=\"text-embedding-3-large\"\n",
    "api_version=\"2023-05-15\"\n",
    "endpoint = \"https://aiservices-forge-test-swe.openai.azure.com/\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key_embedding,  \n",
    "    api_version=api_version,\n",
    "    azure_endpoint = endpoint\n",
    "    )\n",
    "    \n",
    "input = \"Some text to generate embeddings for.\"\n",
    "response = client.embeddings.create(model=deployment_name, input=input)\n",
    "\n",
    "print(f\"Input: {input}\")\n",
    "print(f\"Response: {response.data[0].embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad403c6b",
   "metadata": {},
   "source": [
    "Suggestions for things to try out later on:\n",
    "1. Search Huggingface for some models that looks interesting and try them out. You can also use th Huggingface portal \"Inference API\" directly if you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09131508",
   "metadata": {},
   "source": [
    "## Compare how fine-tuned model and general purpose model can be used for the same task\n",
    "\n",
    "We can classify text into predefined classes by\n",
    "1. Using a general purpose model and prompting to guide the model to do the specific task (classification). We can use for instance the above GPT-3.5 and GPT-4o-mini models for this.\n",
    "2. Using a purpose-built model that is trained to do the specific task. One of the purpose-built models for classification in `lvwerra/distilbert-imdb` that we tried out already above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c11af",
   "metadata": {},
   "source": [
    "## Test these two approaches with some \"dummy\" imdb test data\n",
    "\n",
    "We load a dataset containing Imdb reviews with the review sentiment. Each review is labelled with `0` and `1` where `0` denotes negative review sentiment and `1` positive review sentiment.\n",
    "\n",
    "We use a subset of the original Imdb dataset to limit the traffic to our test endpoint during this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c178b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Imdb dataset from Hugging Face\n",
    "dataset = load_dataset(\"Aappo/imdb_subset\")\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa914434",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Check that you understand what the dataset contains. Hint: Dataset is a dictionary, so check the content with `dataset[\"test\"]` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b18b75",
   "metadata": {},
   "source": [
    "## Classification through prompting\n",
    "\n",
    "Let's see how well the GPT-3.5 model is able to do simple text classification when prompted.\n",
    "\n",
    "First let's defined two auxiliary functions. First one generates the prompt, which is simply:\n",
    "\n",
    "```\n",
    "Task: Classify the text into the classes: negative, positive or neutral.\n",
    "\n",
    "Text: Some text that I want to classify\n",
    "```\n",
    "\n",
    "The second one encodes the model output into integers that correspond to the dataset labels, i.e. `0 = negative sentiment` and `1 = positive sentiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5234f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_CLASSES = [\"negative\",\"positive\", \"neutral\"]\n",
    "\n",
    "def prompt(text):\n",
    "    return \"Task: Classify the text into the classes: \" + \", \".join(SENTIMENT_CLASSES[:-1]) + f\" or {SENTIMENT_CLASSES[-1]}. Text: {text}\"\n",
    "\n",
    "def encode_response(response):\n",
    "    response_string = response.choices[0].text.lower()\n",
    "    matches = [c in response_string for c in SENTIMENT_CLASSES]\n",
    "    if sum(matches) != 1:\n",
    "        print(f\"Invalid response: {response_string}\")\n",
    "        return -1\n",
    "    return matches.index(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e58c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_name=\"gpt-35-turbo-instruct\"\n",
    "predicted_labels = []\n",
    "for i in range(dataset[\"test\"].shape[0]):\n",
    "    text = dataset[\"test\"][\"text\"][i]\n",
    "    true_label = dataset[\"test\"][\"label\"][i]\n",
    "    try:\n",
    "        response = client.completions.create(model=deployment_name, prompt=prompt(text), max_tokens=5)\n",
    "        encoded_response = encode_response(response)\n",
    "        print(\"\\ntext:\", text)\n",
    "        print(\"true_label\", true_label)\n",
    "        print(\"response\", encoded_response)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        encoded_response = -1\n",
    "    predicted_labels.append(encoded_response)\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy_score(dataset[\"test\"][\"label\"], predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb944074",
   "metadata": {},
   "source": [
    "## Classification by using a purpose-built model\n",
    "\n",
    "Let's compare the accuracy with the accuracy of a fine-tuned model. HuggingFace has several different models that are fine-tuned to predict the sentiment of Imdb reviews. Select the model `lvwerra/distilbert-imdb` and calculate the model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707a6dff",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Check the model description from HuggingFace. What is the documented accuracy of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f09cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda022cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_tokens = 512\n",
    "predicted_labels_finetuned = pipe([dataset[\"test\"][\"text\"][i][:n_max_tokens] for i in range(dataset[\"test\"].shape[0])])\n",
    "predicted_labels_finetuned_encoded = [1 if item[\"label\"] == \"POSITIVE\" else 0 for item in predicted_labels_finetuned]\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(dataset[\"test\"][\"label\"], predicted_labels_finetuned_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71318146",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "1. Does the prompting approach and fine-tuned approach have the same difficulties in determining the sentiment (are the same texts classified incorrectly)?\n",
    "2. Can you improve the prompt somehow so that you could get the faulty classifications by the GPT-3.5 models corrected?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
