{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aappopulkkinen/repos/llm-advanced-prompting-public/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline\n",
    ")\n",
    "from typing import List\n",
    "from datasets import load_dataset\n",
    "from openai import AzureOpenAI\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import pipeline\n",
    "import os\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import torch\n",
    "from torch import cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"numind/NuExtract-tiny-v1.5\"\n",
    "device=\"mps\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(text):\n",
    "    template = \"\"\"{\n",
    "    \"Model\": {\n",
    "        \"Name\": \"\",\n",
    "        \"Number of parameters\": \"\",\n",
    "        \"Number of max token\": \"\",\n",
    "        \"Architecture\": []\n",
    "    },\n",
    "    \"Usage\": {\n",
    "        \"Use case\": [],\n",
    "        \"Licence\": \"\"\n",
    "    }\n",
    "}\"\"\"\n",
    "    prompt = f\"\"\"<|input|>\\n### Template:\\n{template}\\n### Text:\\n{text}\\n\\n<|output|>\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
    "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
    "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
    "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
    "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
    "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
    "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
    "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
    "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
    "Code: <https://github.com/mistralai/mistral-src>\n",
    "Webpage: <https://mistral.ai/news/announcing-mistral-7b/>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|input|>\\n### Template:\\n{\\n    \"Model\": {\\n        \"Name\": \"\",\\n        \"Number of parameters\": \"\",\\n        \"Number of max token\": \"\",\\n        \"Architecture\": []\\n    },\\n    \"Usage\": {\\n        \"Use case\": [],\\n        \"Licence\": \"\"\\n    }\\n}\\n### Text:\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: <https://github.com/mistralai/mistral-src>\\nWebpage: <https://mistral.ai/news/announcing-mistral-7b/>\\n\\n<|output|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prompt(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt_encoding = tokenizer(get_prompt(text), return_tensors=\"pt\", truncation=True, padding=True, max_length=10000).to(model.device)\n",
    "pred_ids = model.generate(**prompt_encoding, max_new_tokens=4000)\n",
    "decoded_output = tokenizer.decode(pred_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Model': {'Name': 'Mistral 7B',\n",
       "  'Number of parameters': '7–billion',\n",
       "  'Number of max token': '',\n",
       "  'Architecture': ['grouped-query attention (GQA)',\n",
       "   'sliding window attention (SWA)']},\n",
       " 'Usage': {'Use case': ['superior performance and efficiency',\n",
       "   'reasoning',\n",
       "   'mathematics',\n",
       "   'code generation'],\n",
       "  'Licence': 'Apache 2.0'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(decoded_output.split(\"<|output|>\")[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
